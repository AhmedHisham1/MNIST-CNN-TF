{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_MNIST_TF_playground.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1x0GsBqVvKjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YqxG-m4NIo3v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "dc47d3f8-7faf-4d01-fb84-62bd4e4e73b5"
      },
      "cell_type": "code",
      "source": [
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oz4CUCxgevhd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_accuracy(v_xs, v_ys):\n",
        "    global prediction\n",
        "    \n",
        "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
        "    # Feeding xs=v_xs and keep_prob=1 to calculate (prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2))\n",
        "    # where (h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)) >>> that's why we need to feed keep_prob, also (h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1))\n",
        "    # and (h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64]))...etc, until we reach (xs = tf.placeholder(tf.float32, [None, 784])/255.)\n",
        "    # and (keep_prob = tf.placeholder(tf.float32)). so basically we are feeding the inputs of the CNN which are v_xs in this case, and feeding\n",
        "    # the hyperparameter keep_prob as 1 and then forward propagation runs step by step until it predicts the input images' labels.\n",
        "    \n",
        "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1)) \n",
        "    # tf.equal returns the truth value of tf.argmax(y_pre,1) == tf.argmax(v_ys,1) as a tensor of type bool\n",
        "    # note: tf.argmax returns the index with the largest value across axes of a tensor\n",
        "    \n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    # tf.cast(correct_prediction, tf.float32) casts a tensor to a new type(tf.float32 in this case). Because correct prediction was a tensor of type bool.\n",
        "    # tf.reduce_mean(...) computes the mean of elements across dimensions of a tensor. note: default value for axis=0, so it returns the mean of each column(one column anyway)\n",
        "    \n",
        "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
        "    # feeding the test_x and test_y values and the keep_prob hyperparameter to run the model and evaluate the accuracy; \n",
        "    # where as mentioned above (accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))\n",
        "    # also: correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1)) >>> that's why we feed the test_y (v_ys)\n",
        "    # so the session runs and calculates the accuracy and stores it in the result tensor, which we then return.\n",
        "    \n",
        "    return result\n",
        "  \n",
        "  \n",
        "def print_confusion_matrix(v_xs, v_ys):\n",
        "    # Get the true classifications for the test-set.\n",
        "    cls_true = np.argmax(v_ys, 1)\n",
        "    # Get the predicted classifications for the test-set.\n",
        "    cls_pred = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
        "    cls_pred = np.argmax(cls_pred, 1)\n",
        "    # Get the confusion matrix using sklearn.\n",
        "    cm = confusion_matrix(y_true=cls_true, y_pred=cls_pred)\n",
        "#     cm = tf.confusion_matrix(labels=cls_true, predictions=cls_pred)\n",
        "    \n",
        "    # Print the confusion matrix as text.\n",
        "    # print(cm)\n",
        "    # Plot the confusion matrix as an image.\n",
        "    \n",
        "    plt.figure(1)\n",
        "#     plt.subplot(121)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.tight_layout()\n",
        "    tick_marks = np.arange(10)\n",
        "    plt.xticks(tick_marks, range(10))\n",
        "    plt.yticks(tick_marks, range(10))\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "\n",
        "def test(image_no):\n",
        "    if image_no <= 10000:\n",
        "        global prediction\n",
        "        image = np.reshape(mnist.test.images[image_no], [28,28])\n",
        "        plt.figure(2)\n",
        "#         plt.subplot(122)\n",
        "        plt.imshow(image)\n",
        "        print('\\n\\nPrediction for test image no {} ='.format(image_no), end=' ')\n",
        "        print(np.argmax(sess.run(prediction, feed_dict={xs: np.reshape(mnist.test.images[image_no], [1, 28*28]), keep_prob: 1})))\n",
        "    else:\n",
        "        print('Image number must be less than 10000')\n",
        "  \n",
        "  \n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    # another way(done by andrew ng):\n",
        "#     W = tf.get_variable(\"W\", shape, initializer=tf.contrib.layers.xavier_initializer(seed = 0))  # just using a different initialization method\n",
        "#     return W\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    # stride [1, x_movement, y_movement, 1]\n",
        "    # Must have strides[0] = strides[3] = 1\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "    # stride [1, x_movement, y_movement, 1]\n",
        "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4MqmjEjzfrR5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# define placeholder for inputs to network\n",
        "xs = tf.placeholder(tf.float32, [None, 784])/255.   # 28x28\n",
        "ys = tf.placeholder(tf.float32, [None, 10])\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "x_image = tf.reshape(xs, [-1, 28, 28, 1])\n",
        "# print(x_image.shape)  # [n_samples, 28,28,1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RRR3byApgzJ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## conv1 layer ##\n",
        "W_conv1 = weight_variable([5,5, 1,32]) # patch 5x5, in size 1, out size 32\n",
        "b_conv1 = bias_variable([32])\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 28x28x32\n",
        "h_pool1 = max_pool_2x2(h_conv1)                                         # output size 14x14x32\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uvw1hiZxg1kX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## conv2 layer ##\n",
        "W_conv2 = weight_variable([5,5, 32, 64]) # patch 5x5, in size 32, out size 64\n",
        "b_conv2 = bias_variable([64])\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 14x14x64\n",
        "h_pool2 = max_pool_2x2(h_conv2)                                         # output size 7x7x64\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xk_Atmr6g4eI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## fc1 layer ##\n",
        "W_fc1 = weight_variable([7*7*64, 1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "# [n_samples, 7, 7, 64] ->> [n_samples, 7*7*64]\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vXKUWSZeg68N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## fc2 layer ##\n",
        "W_fc2 = weight_variable([1024, 10])\n",
        "b_fc2 = bias_variable([10])\n",
        "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WEYrx4w7g84g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# the error between prediction and real data\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),\n",
        "                                              reduction_indices=[1]))       # loss\n",
        "train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M7Ntm_X3hBwj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "bf9b25cf-d8f3-41b8-9d99-f2b4cb10d041"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "print('Batch Size = 100 training examples\\n1 Epoch = 600 batches = the whole 60000 training examples available in the MNIST dataset')\n",
        "\n",
        "num_epochs = 1\n",
        "for i in range(600 * num_epochs):\n",
        "    X_batch, Y_batch = mnist.train.next_batch(100)\n",
        "    #I defined every batch to be of 100 training examples, and since the mnist dataset has 60000 training examples, therefore every 600 batchs define an entire epoch\n",
        "    sess.run(train_step, feed_dict={xs: X_batch, ys: Y_batch, keep_prob: 0.5})\n",
        "    if i % 600 == 0 and i != 0:\n",
        "        # testing on the whole 10000 testing examples available in this dataset\n",
        "        print('Accuracy for {} epoch:'.format((i / 600)), end=' ')\n",
        "        print(compute_accuracy(mnist.test.images[:10000], mnist.test.labels[:10000]))\n",
        "        print('Confusion Matrix:')\n",
        "        print_confusion_matrix(mnist.test.images[:10000], mnist.test.labels[:10000])\n",
        "    elif i == (600*num_epochs-1):\n",
        "        print('Accuracy for {} epoch:'.format(( (i+1) / 600)), end=' ')\n",
        "        print(compute_accuracy(mnist.test.images[:10000], mnist.test.labels[:10000]))\n",
        "        print('Confusion Matrix:')\n",
        "        print_confusion_matrix(mnist.test.images[:10000], mnist.test.labels[:10000])\n",
        "\n",
        "        \n",
        "# to show the progress for every 50 batchs instead of the full epochs uncomment this:\n",
        "\n",
        "# num_epochs = 1\n",
        "# for i in range(600 * num_epochs):\n",
        "#     X_batch, Y_batch = mnist.train.next_batch(100)\n",
        "#     #we define every batch to be of 100 training examples, and since the mnist dataset has 60000 training examples, therefore every 600 batchs define an entire epoch\n",
        "#     sess.run(train_step, feed_dict={xs: X_batch, ys: Y_batch, keep_prob: 0.5})\n",
        "#     if i % 50 == 0:\n",
        "#         print('Accuracy for {} batchs:'.format((i / 50)), end=' ')\n",
        "#         print(compute_accuracy(mnist.test.images[:10000], mnist.test.labels[:10000]))\n",
        "#         print('Confusion Matrix:')\n",
        "#         print_confusion_matrix(mnist.test.images[:10000], mnist.test.labels[:10000])\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Batch Size = 100 training examples\n",
            "1 Epoch = 600 batches = the whole 60000 training examples available in the MNIST dataset\n",
            "Accuracy for 1.0 epoch: 0.9791\n",
            "Confusion Matrix:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASIAAAEmCAYAAADGL52gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM4gkVff/sH6/bj021foAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3ce3b91e80>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "wUS6U0VnmpN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "d28331ec-b486-4de3-c4e3-55aade5dafe4"
      },
      "cell_type": "code",
      "source": [
        "# Testing the images predictions\n",
        "\n",
        "test_image_no = 1285   #tweak this number to test different images from the test set.\n",
        "test(test_image_no)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Prediction for test image no 1285 = 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngf8HBTQl8fptmK0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3ce21ffa58>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "jW7ByK8-m8M9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}